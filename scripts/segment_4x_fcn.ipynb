{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef274ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from glob import glob\n",
    "from tifffile import imread\n",
    "#from cellpose import models #in case models are nto downloaded yet\n",
    "\n",
    "# DISPLAY RESULTS imports\n",
    "#from cellpose import plot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.ndimage import zoom\n",
    "import skimage.io\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from torchvision.models.segmentation.fcn import FCNHead\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from scipy.ndimage import morphology\n",
    "sys.path.append(os.path.join(sys.path[0]))  # To find local version of the library\n",
    "\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "import torch\n",
    "from skimage.color import rgb2gray, label2rgb\n",
    "from skimage import io,exposure, feature, filters, io, measure, morphology, restoration, segmentation, transform, util, data, color\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "import os\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "#load image\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from skimage.morphology import remove_small_objects,dilation, erosion, ball  \n",
    "from skimage import (exposure, feature, filters, io, measure,\n",
    "                      morphology, restoration, segmentation, transform,\n",
    "                      util)\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from scipy.ndimage import zoom\n",
    "import copy\n",
    "import os\n",
    "import scipy.ndimage\n",
    "from skimage import measure, morphology\n",
    "import gc\n",
    "from skimage.util import invert\n",
    "from skimage.morphology import remove_small_objects,dilation, erosion, ball  \n",
    "from skimage import (exposure, feature, filters, io, measure,\n",
    "                      morphology, restoration, segmentation, transform,\n",
    "                      util)\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "from scipy.ndimage import zoom\n",
    "import copy\n",
    "import os\n",
    "import scipy.ndimage\n",
    "from skimage import measure, morphology\n",
    "import gc\n",
    "from skimage.util import invert\n",
    "import pandas as pd\n",
    "from vedo import *\n",
    "import pyvista as pv\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.transform import rescale\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "from torchvision.models.segmentation.fcn import FCNHead\n",
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "\n",
    "\n",
    "\n",
    "class Material:\n",
    " \n",
    "  def __init__(self, name, input_rgb_vals, output_val, confidence_threshold=0):\n",
    "    self.name = name\n",
    "    self.input_rgb_vals = input_rgb_vals\n",
    "    self.output_val = output_val\n",
    "    self.confidence_threshold = confidence_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28f67aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "885714f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_out=\"E:\\\\ANSTO_20960\\\\models\\\\\"\n",
    "\n",
    "model_group='devin_cropped/'\n",
    "current_model_name = 'rhizosheath_ai'\n",
    "model_number=\"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "037128b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#root or not root\n",
    "#Creating a list of materials so we can iterate through it\n",
    "materials = [\n",
    "            Material(\"kapton\", [36,36,36], 36 , 0.3),\n",
    "            Material(\"root\", [73,73,73], 73, 0.4),\n",
    "            Material(\"primary\", [109,109,109], 109, 0.5),\n",
    "            Material(\"mixed\", [146,146,146], 146, 0.5),\n",
    "            Material(\"root_hair\", [182,182,182], 182, 0.05),\n",
    "            Material(\"pore\", [219,219,219], 219, 0.5),\n",
    "            Material(\"organic\", [255,255,255], 255, 0.6)]\n",
    "\n",
    "num_materials =len(materials)\n",
    "#Decrease scale to decrease VRAM usage; if you run out of VRAM during traing, restartyour runtime and down scale your images\n",
    "scale=1\n",
    "#Input model directory\n",
    "models_directory = model_out\n",
    "#Input the name you want to use for your group of models\n",
    "model_group=model_group\n",
    "current_model_name = current_model_name\n",
    "model_number=model_number\n",
    "\n",
    "\n",
    "###############################\n",
    "# Load FCN\n",
    "model=torchvision.models.segmentation.fcn_resnet101(pretrained=False)\n",
    "model.classifier=FCNHead(2048, num_materials)\n",
    "device = torch.device('cuda')\n",
    "outputs=[]\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(models_directory+model_group + current_model_name+model_number+'.pth'), strict=False)\n",
    "model.train()\n",
    "mean=3.0020e-15 #This is silly just copy and paste value from model train - this should be saved as a text file ?\n",
    "std=1\n",
    "\n",
    "# # Load deep lab v3\n",
    "# model=torchvision.models.segmentation.fcn_resnet101(pretrained=False)\n",
    "# model.classifier=FCNHead(2048, num_materials)\n",
    "# device = torch.device('cuda')\n",
    "# outputs=[]\n",
    "# model.to(device)\n",
    "# model.load_state_dict(torch.load(models_directory+model_group + current_model_name+model_number+'.pth'), strict=False)\n",
    "# model.train()\n",
    "# mean=3.0020e-15 #This is silly just copy and paste value from model train - this should be saved as a text file ?\n",
    "# std=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b841f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2134457",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E:\\\\ANSTO_20960\\\\rhizo_ai\\\\XY_SLICES_CROPPED\\\\recon_076_W16_S4_P.tif\\\\'\n",
      " 'E:\\\\ANSTO_20960\\\\rhizo_ai\\\\XY_SLICES_CROPPED\\\\recon_077_W1_S4_P.tif\\\\'\n",
      " 'E:\\\\ANSTO_20960\\\\rhizo_ai\\\\XY_SLICES_CROPPED\\\\recon_078_WT_S7_P.tif\\\\'\n",
      " 'E:\\\\ANSTO_20960\\\\rhizo_ai\\\\XY_SLICES_CROPPED\\\\recon_079_BALD_S1_P.tif\\\\'\n",
      " 'E:\\\\ANSTO_20960\\\\rhizo_ai\\\\XY_SLICES_CROPPED\\\\recon_080_W16_S3_P.tif\\\\'\n",
      " 'E:\\\\ANSTO_20960\\\\rhizo_ai\\\\XY_SLICES_CROPPED\\\\recon_081_W1_S2_P.tif\\\\'\n",
      " 'E:\\\\ANSTO_20960\\\\rhizo_ai\\\\XY_SLICES_CROPPED\\\\recon_082_W1_S1_P.tif\\\\'\n",
      " 'E:\\\\ANSTO_20960\\\\rhizo_ai\\\\XY_SLICES_CROPPED\\\\recon_083_BALD_S4_P.tif\\\\'\n",
      " 'E:\\\\ANSTO_20960\\\\rhizo_ai\\\\XY_SLICES_CROPPED\\\\recon_084_W16_S1_P.tif\\\\'\n",
      " 'E:\\\\ANSTO_20960\\\\rhizo_ai\\\\XY_SLICES_CROPPED\\\\recon_085_W1_S3_P.tif\\\\'\n",
      " 'E:\\\\ANSTO_20960\\\\rhizo_ai\\\\XY_SLICES_CROPPED\\\\recon_086_WT_S9_P.tif\\\\'\n",
      " 'E:\\\\ANSTO_20960\\\\rhizo_ai\\\\XY_SLICES_CROPPED\\\\recon_087_BALDY_S7_P.tif\\\\'\n",
      " 'E:\\\\ANSTO_20960\\\\rhizo_ai\\\\XY_SLICES_CROPPED\\\\recon_088_W16_S2_P.tif\\\\'\n",
      " 'E:\\\\ANSTO_20960\\\\rhizo_ai\\\\XY_SLICES_CROPPED\\\\recon_089_WT_S2_P.tif\\\\']\n"
     ]
    }
   ],
   "source": [
    "#Part 2 Segment using AI (Segment)\n",
    "#Loop through folders#######################################\n",
    "ct_folder_path_name ='E:\\\\ANSTO_20960\\\\rhizo_ai\\\\XY_SLICES_CROPPED\\\\'\n",
    "\n",
    "ct_folder_paths=np.array((glob(ct_folder_path_name+\"*\\\\\", recursive = True)))\n",
    "########\n",
    "ct_folder_paths = ct_folder_paths[ct_folder_paths != 'E:\\\\ANSTO_20960\\\\rhizo_ai\\\\XY_SLICES_CROPPED\\\\recon_072_W16_S5_P.tif\\\\']\n",
    "ct_folder_paths = ct_folder_paths[ct_folder_paths != 'E:\\\\ANSTO_20960\\\\rhizo_ai\\\\XY_SLICES_CROPPED\\\\recon_073_W1_S5_P.tif\\\\']\n",
    "ct_folder_paths = ct_folder_paths[ct_folder_paths != 'E:\\\\ANSTO_20960\\\\rhizo_ai\\\\XY_SLICES_CROPPED\\\\recon_074_WT_S10_P.tif\\\\']\n",
    "print (ct_folder_paths)\n",
    "############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a61e73c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ct_folder_paths=ct_folder_paths[ct_folder_paths == \"E:\\\\ANSTO_20960\\\\tiff_stacks_out\\\\4x_white_beam\\\\XY\\\\recon_082_W1_S1_P.tif\\\\\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a379133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2af05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85d630af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recon_076_W16_S4_P.tif\n",
      "recon_077_W1_S4_P.tif\n",
      "recon_078_WT_S7_P.tif\n",
      "recon_079_BALD_S1_P.tif\n",
      "recon_080_W16_S3_P.tif\n",
      "recon_081_W1_S2_P.tif\n",
      "recon_082_W1_S1_P.tif\n",
      "recon_083_BALD_S4_P.tif\n",
      "recon_084_W16_S1_P.tif\n",
      "recon_085_W1_S3_P.tif\n",
      "recon_086_WT_S9_P.tif\n",
      "recon_087_BALDY_S7_P.tif\n",
      "recon_088_W16_S2_P.tif\n",
      "recon_089_WT_S2_P.tif\n"
     ]
    }
   ],
   "source": [
    "#########################################\n",
    "output_base = \"E:\\\\ANSTO_20960\\\\rhizo_ai\\\\segmented_out\\\\\"\n",
    "for folder_x in ct_folder_paths:\n",
    "    #print (folder_x)\n",
    "    IMAGE_ID = folder_x.replace(ct_folder_path_name , \"\")\n",
    "    IMAGE_ID = IMAGE_ID.replace('\\\\', '')\n",
    "    print (IMAGE_ID)\n",
    "    output_directory = output_base+IMAGE_ID+\"\\\\\"\n",
    "    if os.path.isdir(output_directory):\n",
    "        out=(output_directory)\n",
    "    else:\n",
    "        out=os.mkdir(output_directory)\n",
    "        \n",
    "    inference_directory= folder_x\n",
    "    proceeding=\"lice_\"\n",
    "    following=\".tif\"\n",
    "    dir_name = inference_directory\n",
    "    filenames = os.listdir(dir_name)\n",
    "    sort_idx = np.argsort([(int(filename.split(proceeding)[1].split(following)[0])) for filename in filenames])\n",
    "    for i in sort_idx:\n",
    "        #makes new directory called \"(directory name here) + name in red\" that your new images go into\n",
    "        new_dir_name = output_directory\n",
    "        if not os.path.exists(new_dir_name):\n",
    "          os.makedirs(new_dir_name)\n",
    "\n",
    "        for mat in materials:\n",
    "          new_dir_name_mat= new_dir_name + mat.name\n",
    "          if not os.path.exists(new_dir_name_mat):\n",
    "            os.makedirs(new_dir_name_mat)\n",
    "        filename = filenames[i]\n",
    "\n",
    "        image = Image.open(dir_name +'/'+ filenames[i])\n",
    "        image1 = (image)\n",
    "        image1 = np.array(image1, dtype='ubyte')\n",
    "        zeros=np.zeros_like(image1)\n",
    "\n",
    "        w, h = image.size\n",
    "        #print(image.size)\n",
    "        #!!!!!!!!!!!!!!!!!!!!Make sure scale matches!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        scale=scale\n",
    "        newW, newH = int(scale * w), int(scale * h)\n",
    "        image=image.resize((newW, newH))\n",
    "        image_resize=image\n",
    "        image=np.array(image)\n",
    "        zeros2=np.zeros_like(image)\n",
    "        new_im=np.zeros((3, newH, newW))\n",
    "        new_im[0,:,:]=image\n",
    "        new_im[1,:,:]=image\n",
    "        new_im[2,:,:]=image\n",
    "        image=new_im\n",
    "        image=torch.from_numpy(image)\n",
    "        image=T.Normalize(mean=[mean, mean , mean], std=[std, std , std])(image)\n",
    "        image.unsqueeze_(0)\n",
    "        image = image.to(device=device, dtype=torch.float32)\n",
    "        tic=time.time()\n",
    "        with torch.no_grad():\n",
    "          mask=model(image)['out']\n",
    "          mask=nn.Sigmoid()(mask)\n",
    "        toc=time.time()\n",
    "        #print('time: '+str(toc-tic))\n",
    "\n",
    "        image_rescaled=zeros2\n",
    "        combined_image = np.array(image_rescaled, dtype='ubyte') \n",
    "\n",
    "        list_of_mat_tables = []\n",
    "        for i, mat in enumerate(materials):\n",
    "          mat_mask = mask.cpu().detach().numpy()[0,i,:,:]\n",
    "          mat_mask[mat_mask >= mat.confidence_threshold] = mat.output_val\n",
    "          mat_mask[mat_mask < mat.confidence_threshold] = 0\n",
    "          mat_mask=np.array(mat_mask, dtype='ubyte')\n",
    "          combined_image = np.add(combined_image, mat_mask[:,:])\n",
    "\n",
    "          io.imsave(new_dir_name+'/' + mat.name + '/'+filename.split(following)[0]+'_' + mat.name + \"_mask.png\",\n",
    "                    mat_mask, check_contrast=False)\n",
    "          np_mat = np.array(mat_mask)\n",
    "\n",
    "\n",
    "        io.imsave(new_dir_name+'/'+filename.split(following)[0]+'.png', combined_image, check_contrast=False)   \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0443b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
